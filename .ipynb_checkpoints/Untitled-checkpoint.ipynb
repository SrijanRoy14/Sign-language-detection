{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d033b9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.8.0 in c:\\users\\srijan\\appdata\\roaming\\python\\python310\\site-packages (2.8.0)\n",
      "Requirement already satisfied: tensorflow-gpu==2.8.0 in c:\\users\\srijan\\appdata\\roaming\\python\\python310\\site-packages (2.8.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\srijan\\appdata\\roaming\\python\\python310\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: sklearn in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.post1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.7.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (23.1.4)\n",
      "Requirement already satisfied: gast>=0.2.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (3.7.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (1.1.2)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (15.0.6.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (1.23.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (3.19.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (58.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (4.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (1.14.1)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (2.8.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (2.8.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (0.29.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (1.51.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.16.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.3.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (5.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.8.0 tensorflow-gpu==2.8.0 opencv-python sklearn matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94858335",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.9.1.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (1.4.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (23.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (23.1.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (3.7.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (1.23.1)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (4.7.0.72)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (3.19.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad950c89",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                      Version\n",
      "---------------------------- -------------------\n",
      "absl-py                      1.4.0\n",
      "asttokens                    2.2.1\n",
      "astunparse                   1.6.3\n",
      "attrs                        23.1.0\n",
      "backcall                     0.2.0\n",
      "cachetools                   5.3.1\n",
      "certifi                      2023.5.7\n",
      "cffi                         1.15.1\n",
      "charset-normalizer           3.2.0\n",
      "colorama                     0.4.6\n",
      "comm                         0.1.3\n",
      "contourpy                    1.1.0\n",
      "cycler                       0.11.0\n",
      "debugpy                      1.6.7\n",
      "decorator                    5.1.1\n",
      "executing                    1.2.0\n",
      "flatbuffers                  23.5.26\n",
      "fonttools                    4.41.0\n",
      "gast                         0.5.4\n",
      "google-auth                  2.22.0\n",
      "google-auth-oauthlib         0.4.6\n",
      "google-pasta                 0.2.0\n",
      "grpcio                       1.56.0\n",
      "h5py                         3.9.0\n",
      "idna                         3.4\n",
      "ipykernel                    6.24.0\n",
      "ipython                      8.14.0\n",
      "jedi                         0.18.2\n",
      "joblib                       1.3.1\n",
      "jupyter_client               8.3.0\n",
      "jupyter_core                 5.3.1\n",
      "keras                        2.8.0\n",
      "Keras-Preprocessing          1.1.2\n",
      "kiwisolver                   1.4.4\n",
      "libclang                     16.0.6\n",
      "Markdown                     3.4.3\n",
      "MarkupSafe                   2.1.3\n",
      "matplotlib                   3.7.2\n",
      "matplotlib-inline            0.1.6\n",
      "mediapipe                    0.10.2\n",
      "nest-asyncio                 1.5.6\n",
      "numpy                        1.25.1\n",
      "oauthlib                     3.2.2\n",
      "opencv-contrib-python        4.8.0.74\n",
      "opencv-python                4.8.0.74\n",
      "opt-einsum                   3.3.0\n",
      "packaging                    23.1\n",
      "parso                        0.8.3\n",
      "pickleshare                  0.7.5\n",
      "Pillow                       10.0.0\n",
      "pip                          22.0.4\n",
      "platformdirs                 3.9.1\n",
      "prompt-toolkit               3.0.39\n",
      "protobuf                     3.20.3\n",
      "psutil                       5.9.5\n",
      "pure-eval                    0.2.2\n",
      "pyasn1                       0.5.0\n",
      "pyasn1-modules               0.3.0\n",
      "pycparser                    2.21\n",
      "Pygments                     2.15.1\n",
      "pyparsing                    3.0.9\n",
      "python-dateutil              2.8.2\n",
      "pywin32                      306\n",
      "pyzmq                        25.1.0\n",
      "requests                     2.31.0\n",
      "requests-oauthlib            1.3.1\n",
      "rsa                          4.9\n",
      "scikit-learn                 1.3.0\n",
      "scipy                        1.11.1\n",
      "setuptools                   58.1.0\n",
      "six                          1.16.0\n",
      "sklearn                      0.0.post5\n",
      "sounddevice                  0.4.6\n",
      "stack-data                   0.6.2\n",
      "tensorboard                  2.8.0\n",
      "tensorboard-data-server      0.6.1\n",
      "tensorboard-plugin-wit       1.8.1\n",
      "tensorflow                   2.8.0\n",
      "tensorflow-gpu               2.8.0\n",
      "tensorflow-io-gcs-filesystem 0.31.0\n",
      "termcolor                    2.3.0\n",
      "tf-estimator-nightly         2.8.0.dev2021122109\n",
      "threadpoolctl                3.2.0\n",
      "tornado                      6.3.2\n",
      "traitlets                    5.9.0\n",
      "typing_extensions            4.7.1\n",
      "urllib3                      1.26.16\n",
      "wcwidth                      0.2.6\n",
      "Werkzeug                     2.3.6\n",
      "wheel                        0.40.0\n",
      "wrapt                        1.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\SRIJAN\\Dropbox\\PC\\Documents\\Programing\\Projects\\Python\\Deep_learning_proj\\Sign language detection\\signlanguage\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6230542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import time \n",
    "import mediapipe as mp # mediapipe will give us the keypoints from our image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b44231",
   "metadata": {},
   "source": [
    "# Keypoints using Media Pipe Holistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8211742",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic=mp.solutions.holistic #brings in our holistic model- used to make our detection\n",
    "mp_drawing=mp.solutions.drawing_utils #drawing utilities- used to draw them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cbb1481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image,model):\n",
    "    image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)#colous conversion- since opencv reads in BGR and mediapipe RGB\n",
    "    image.flags.writeable=False #image is no longer writable- saves memory\n",
    "    results=model.process(image)#process the keypoints in the image using mediapipe holistic. Basically prediction on the frame\n",
    "    image.flags.writeable=True\n",
    "    image=cv2.cvtColor(image,cv2.COLOR_RGB2BGR)#convert it back to BGR\n",
    "    return image,results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01e1568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results): #helper function to draw the lanmarks on the image\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, \n",
    "                                  mp_holistic.FACEMESH_CONTOURS,mp_drawing.DrawingSpec(color=(80,256,121),thickness=1,circle_radius=1),\n",
    "                                 mp_drawing.DrawingSpec(color=(110,256,121),thickness=1,circle_radius=1))#connects one landmark to another, contor is for the dots and connections is for the lines>\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "518fbec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0) #turns on the webcam, '0' is for webcam\n",
    "#access mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened(): #loop through all the frames\n",
    "        ret,frame=cap.read() #read the feed, feed is read one frame at atime. Frame is the image\n",
    "\n",
    "        #make detections\n",
    "        image,results=mediapipe_detection(frame,holistic)\n",
    "       # we have different types of functions for our results, landmark functions. for ex face landmark detects face keypoints nd left hand landmarks for left hand\n",
    "        \n",
    "        #draw landmarks\n",
    "        draw_landmarks(image,results)\n",
    "        \n",
    "        #show to screen\n",
    "        cv2.imshow('Open Feed',image) #showing the update 'frame'\n",
    "        \n",
    "        \n",
    "        if cv2.waitKey(10)&0xFF==ord('q'): #quit. Q on our keyboard\n",
    "            break\n",
    "    cap.release()#release the frames\n",
    "    cv2.destroyAllWindows()#closes the frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1fdd3f",
   "metadata": {},
   "source": [
    "# Extract Keypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca1cceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose=np.array([[res.x,res.y,res.z,res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "\n",
    "    lh=np.array([[res.x,res.y,res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    #what we are doing here is a bit of an error handling if any of the right hand or left hand array is empty we are filling it with zeroes of same shaep\n",
    "    #we knoe waxh hand landmark has 21 landmar elements comprising of 3 axes so (21*3)\n",
    "    #also list comprehension is used for insertion then the whole is converted to and array and flattened out\n",
    "\n",
    "    rh=np.array([[res.x,res.y,res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    #what we are doing here is a bit of an error handling if any of the right hand or left hand array is empty we are filling it with zeroes of same shaep\n",
    "    #we knoe waxh hand landmark has 21 landmar elements comprising of 3 axes so (21*3)\n",
    "    #also list comprehension is used for insertion then the whole is converted to and array and flattened out\n",
    "\n",
    "    face=np.array([[res.x,res.y,res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    #what we are doing here is a bit of an error handling if any of the right hand or left hand array is empty we are filling it with zeroes of same shaep\n",
    "    #we knoe waxh hand landmark has 468 landmar elements comprising of 3 axes so (468*3)\n",
    "    #also list comprehension is used for insertion then the whole is converted to and array and flattened out\n",
    "    \n",
    "    return np.concatenate([pose,face,lh,rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cabf63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test=extract_keypoints(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "facf71eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('0',result_test)#save the test extracted keypoint result in a file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb95345",
   "metadata": {},
   "source": [
    "# Folders for collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36945876",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=os.path.join('MP_Data')#path for exported keypoint data, numpy arrays\n",
    "\n",
    "actions=np.array(['hello','thanks','iloveyou'])#Actions that we will tryto detect\n",
    "\n",
    "no_seq=30 #30 videos worth of data\n",
    "\n",
    "seq_len=30#each video/seq will have 30 frames\n",
    "\n",
    "#the key diff between object detection and action det that we use multiple friends in the latter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdfac07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4487400"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total len of data\n",
    "1662*30*30*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c22462c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for action in actions:\n",
    "    for seq in range(no_seq):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(data_path,action,str(seq)))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "#creates directiories for eac h action and 30 seq\n",
    "#effectively MP_Data/hello/0.....MP_Data/Hello/29\n",
    "#and similar for the rest two\n",
    "#each seq(Folder 0..29) are gonna inturn contain 30 frames worth of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a0b883",
   "metadata": {},
   "source": [
    "# Collects keypoint values for training and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d8b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0) #turns on the webcam, '0' is for webcam\n",
    "#access mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    #loop thorugh actions\n",
    "    for action in actions:\n",
    "        #loop through sequences aka videos\n",
    "        for seq in range(no_seq):\n",
    "            #loop though video length\n",
    "            for frame_num in range(seq_len):\n",
    "                \n",
    "                \n",
    "        \n",
    "                ret,frame=cap.read() #read the feed, feed is read one frame at atime. Frame is the image\n",
    "\n",
    "                #make detections\n",
    "                image,results=mediapipe_detection(frame,holistic)\n",
    "               # we have different types of functions for our results, landmark functions. for ex face landmark detects face keypoints nd left hand landmarks for left hand\n",
    "                \n",
    "                #draw landmarks\n",
    "                draw_landmarks(image,results)\n",
    "                \n",
    "                #apply wait logic\n",
    "                if frame_num==0:\n",
    "                    cv2.putText(image,'STARTING COLLECTION',(120,200),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),4,cv2.LINE_AA)\n",
    "                    cv2.putText(image,f'Collecting frames for {action} video number {seq}',(15,12),cv2.FONT_HERSHEY_SIMPLEX,0.5,(125,255,178),1,cv2.LINE_AA)\n",
    "                    cv2.waitKey(2000)#if the frame is just starting add a collection break before the next sequence\n",
    "                else:\n",
    "                    cv2.putText(image,f'Collecting frames for {action} video number {seq}',(15,12),cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,0,0),2,cv2.LINE_AA)\n",
    "                \n",
    "                #Export keypoints\n",
    "                keypoints=extract_keypoints(results)\n",
    "                npy_path=os.path.join(data_path,action,str(seq),str(frame_num))\n",
    "                np.save(npy_path,keypoints)\n",
    "                #show to screen\n",
    "                cv2.imshow('Open Feed',image) #showing the update 'frame'\n",
    "        \n",
    "        \n",
    "                if cv2.waitKey(10)&0xFF==ord('q'): #quit. Q on our keyboard\n",
    "                    break\n",
    "    cap.release()#release the frames\n",
    "    cv2.destroyAllWindows()#closes the frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02779ab0",
   "metadata": {},
   "source": [
    "# Preprocess data and create labels and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ed453ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.23.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9c265e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6d02868",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map={label:num for num,label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17d602f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 0, 'thanks': 1, 'iloveyou': 2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1956b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences,labels=[],[] #sequences=30 videos for each action\n",
    "for action in actions:\n",
    "    for seq in range(no_seq):\n",
    "        window=[] #create an empty list for each video to append every 30 frame containing 1662 keypoints\n",
    "        for frame_num in range(seq_len):\n",
    "            res=np.load(os.path.join(data_path,action,str(seq),f'{frame_num}.npy'))#loading up keypoints from each frame\n",
    "            window.append(res)#appending each frame with keypoints\n",
    "        sequences.append(window) #append the 30 frames to a video\n",
    "        labels.append(label_map[action]) #sets the labels for each action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ce2f0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array(sequences) # we got 90 vides(3*30, 30 videos for each action) each video has got 30 frames and eaxch frame has got 1662 keypoints\n",
    "#this is the x labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0e39426",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=to_categorical(labels).astype(int) #y labes using keras to convert the labels to flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eb3b54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y #100 represents hello, 010 thank you, 001 i love you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46a4383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "...     x, y, test_size=0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfd44a8",
   "metadata": {},
   "source": [
    "# Train the LSTM NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "250fbe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import LSTM,Dense,Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard,EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "21d73f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir=os.path.join('logs')\n",
    "tb_callback=TensorBoard(log_dir=log_dir) #create a call back directory\n",
    "early=EarlyStopping(monitor='categorical_accuracy',patience=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e35aa536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(LSTM(64,return_sequences=True,activation='relu',input_shape=(30,1662))) #since there are 30 frames in each video with 1662 keypoints we will use it as input shape\n",
    "#return_seq is used as memory\n",
    "model.add(LSTM(128,return_sequences=True,activation='relu'))\n",
    "model.add(Dropout(0.02))\n",
    "model.add(LSTM(64,return_sequences=False,activation='relu'))\n",
    "#model.add(Dropout(0.22))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dropout(0.018))\n",
    "model.add(Dense(actions.shape[0],activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c304709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c527a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train,epochs=200,callbacks=[tb_callback]) #trained till 124 epochs as accuraccy was pretty good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0786a130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 30, 64)            442112    \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 30, 128)           98816     \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 596,675\n",
      "Trainable params: 596,675\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5556aadb",
   "metadata": {},
   "source": [
    "# Make predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "004db441",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=action_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22500bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.4855652e-01, 3.1455502e-01, 3.3688846e-01],\n",
       "       [9.7081135e-04, 1.6120598e-02, 9.8290861e-01],\n",
       "       [1.8294715e-05, 9.9941278e-01, 5.6892732e-04],\n",
       "       [1.6121319e-05, 9.9933547e-01, 6.4838812e-04],\n",
       "       [8.6418194e-01, 1.2694535e-01, 8.8727018e-03]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "462faff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(res[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f8160e8-8546-423f-be33-7d70b6748917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6b191603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(y_test[4])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3a2b79",
   "metadata": {},
   "source": [
    "#### Save the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4579b32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action.h5')#save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a34040-3eba-458c-ade3-938aec85bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9b66d2d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "action_model = tf.keras.models.load_model('action.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049c9d1e",
   "metadata": {},
   "source": [
    "# Evaluating confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf2f7be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e5bff594",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat=action_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c1b85be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue=np.argmax(y_test,axis=1).tolist()\n",
    "yhat=np.argmax(yhat,axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99c9875a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3, 0],\n",
       "        [2, 0]],\n",
       "\n",
       "       [[0, 4],\n",
       "        [0, 1]],\n",
       "\n",
       "       [[3, 0],\n",
       "        [2, 0]]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue,yhat) #represents the matrices for 3 categories hello,thankyou and iloveyou\n",
    "#matrix is arranged in true +ve,False +ve,False -ve,True n-ve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6ebf5cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01afd05",
   "metadata": {},
   "source": [
    "# Test in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f7731f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new detection variables\n",
    "sequence=[]\n",
    "sentence=[]\n",
    "threshold=0.8\n",
    "\n",
    "\n",
    "\n",
    "cap=cv2.VideoCapture(0) #turns on the webcam, '0' is for webcam\n",
    "#access mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened(): #loop through all the frames\n",
    "        ret,frame=cap.read() #read the feed, feed is read one frame at atime. Frame is the image\n",
    "\n",
    "        #make detections\n",
    "        image,results=mediapipe_detection(frame,holistic)\n",
    "        #print(results)\n",
    "       # we have different types of functions for our results, landmark functions. for ex face landmark detects face keypoints nd left hand landmarks for left hand\n",
    "        \n",
    "        #draw landmarks\n",
    "        draw_landmarks(image,results)\n",
    "        \n",
    "        #prediction logic\n",
    "        keypoints=extract_keypoints(results)\n",
    "        \n",
    "        sequence.insert(0,keypoints)\n",
    "        sequence=sequence[:30]\n",
    "        if len(sequence)==30:\n",
    "            res=action_model.predict(np.expand_dims(sequence,axis=0))[0]\n",
    "            #print(actions[np.argmax(res)])\n",
    "\n",
    "        #viz. logic\n",
    "        if res[np.argmax(res)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "        if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            #image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "       \n",
    "        \n",
    "        #show to screen\n",
    "        cv2.imshow('Open Feed',image) #showing the update 'frame'\n",
    "        \n",
    "        \n",
    "        if cv2.waitKey(10)&0xFF==ord('q'): #quit. Q on our keyboard\n",
    "            break\n",
    "    cap.release()#release the frames\n",
    "    cv2.destroyAllWindows()#closes the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbb1620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e56309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signlanguage",
   "language": "python",
   "name": "signlanguage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
