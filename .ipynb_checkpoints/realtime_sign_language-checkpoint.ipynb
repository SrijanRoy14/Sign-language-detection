{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d033b9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.8.0 in c:\\users\\srijan\\appdata\\roaming\\python\\python310\\site-packages (2.8.0)\n",
      "Requirement already satisfied: tensorflow-gpu==2.8.0 in c:\\users\\srijan\\appdata\\roaming\\python\\python310\\site-packages (2.8.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\srijan\\appdata\\roaming\\python\\python310\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: sklearn in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.post1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.7.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (23.1.4)\n",
      "Requirement already satisfied: gast>=0.2.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (3.7.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (1.1.2)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (15.0.6.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (1.23.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (3.19.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (58.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (4.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (1.14.1)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (2.8.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (2.8.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (0.29.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.8.0) (1.51.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.16.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.3.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (5.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.8.0 tensorflow-gpu==2.8.0 opencv-python sklearn matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94858335",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.9.1.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (1.4.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (23.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (23.1.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (3.7.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (1.23.1)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (4.7.0.72)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (3.19.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad950c89",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                      Version\n",
      "---------------------------- -------------------\n",
      "absl-py                      1.4.0\n",
      "asttokens                    2.2.1\n",
      "astunparse                   1.6.3\n",
      "attrs                        23.1.0\n",
      "backcall                     0.2.0\n",
      "cachetools                   5.3.1\n",
      "certifi                      2023.5.7\n",
      "cffi                         1.15.1\n",
      "charset-normalizer           3.2.0\n",
      "colorama                     0.4.6\n",
      "comm                         0.1.3\n",
      "contourpy                    1.1.0\n",
      "cycler                       0.11.0\n",
      "debugpy                      1.6.7\n",
      "decorator                    5.1.1\n",
      "executing                    1.2.0\n",
      "flatbuffers                  23.5.26\n",
      "fonttools                    4.41.0\n",
      "gast                         0.5.4\n",
      "google-auth                  2.22.0\n",
      "google-auth-oauthlib         0.4.6\n",
      "google-pasta                 0.2.0\n",
      "grpcio                       1.56.0\n",
      "h5py                         3.9.0\n",
      "idna                         3.4\n",
      "ipykernel                    6.24.0\n",
      "ipython                      8.14.0\n",
      "jedi                         0.18.2\n",
      "joblib                       1.3.1\n",
      "jupyter_client               8.3.0\n",
      "jupyter_core                 5.3.1\n",
      "keras                        2.8.0\n",
      "Keras-Preprocessing          1.1.2\n",
      "kiwisolver                   1.4.4\n",
      "libclang                     16.0.6\n",
      "Markdown                     3.4.3\n",
      "MarkupSafe                   2.1.3\n",
      "matplotlib                   3.7.2\n",
      "matplotlib-inline            0.1.6\n",
      "mediapipe                    0.10.2\n",
      "nest-asyncio                 1.5.6\n",
      "numpy                        1.25.1\n",
      "oauthlib                     3.2.2\n",
      "opencv-contrib-python        4.8.0.74\n",
      "opencv-python                4.8.0.74\n",
      "opt-einsum                   3.3.0\n",
      "packaging                    23.1\n",
      "parso                        0.8.3\n",
      "pickleshare                  0.7.5\n",
      "Pillow                       10.0.0\n",
      "pip                          22.0.4\n",
      "platformdirs                 3.9.1\n",
      "prompt-toolkit               3.0.39\n",
      "protobuf                     3.20.3\n",
      "psutil                       5.9.5\n",
      "pure-eval                    0.2.2\n",
      "pyasn1                       0.5.0\n",
      "pyasn1-modules               0.3.0\n",
      "pycparser                    2.21\n",
      "Pygments                     2.15.1\n",
      "pyparsing                    3.0.9\n",
      "python-dateutil              2.8.2\n",
      "pywin32                      306\n",
      "pyzmq                        25.1.0\n",
      "requests                     2.31.0\n",
      "requests-oauthlib            1.3.1\n",
      "rsa                          4.9\n",
      "scikit-learn                 1.3.0\n",
      "scipy                        1.11.1\n",
      "setuptools                   58.1.0\n",
      "six                          1.16.0\n",
      "sklearn                      0.0.post5\n",
      "sounddevice                  0.4.6\n",
      "stack-data                   0.6.2\n",
      "tensorboard                  2.8.0\n",
      "tensorboard-data-server      0.6.1\n",
      "tensorboard-plugin-wit       1.8.1\n",
      "tensorflow                   2.8.0\n",
      "tensorflow-gpu               2.8.0\n",
      "tensorflow-io-gcs-filesystem 0.31.0\n",
      "termcolor                    2.3.0\n",
      "tf-estimator-nightly         2.8.0.dev2021122109\n",
      "threadpoolctl                3.2.0\n",
      "tornado                      6.3.2\n",
      "traitlets                    5.9.0\n",
      "typing_extensions            4.7.1\n",
      "urllib3                      1.26.16\n",
      "wcwidth                      0.2.6\n",
      "Werkzeug                     2.3.6\n",
      "wheel                        0.40.0\n",
      "wrapt                        1.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\SRIJAN\\Dropbox\\PC\\Documents\\Programing\\Projects\\Python\\Deep_learning_proj\\Sign language detection\\signlanguage\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6230542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import time \n",
    "import mediapipe as mp # mediapipe will give us the keypoints from our image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b44231",
   "metadata": {},
   "source": [
    "# Keypoints using Media Pipe Holistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8211742",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic=mp.solutions.holistic #brings in our holistic model- used to make our detection\n",
    "mp_drawing=mp.solutions.drawing_utils #drawing utilities- used to draw them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cbb1481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image,model):\n",
    "    image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)#colous conversion- since opencv reads in BGR and mediapipe RGB\n",
    "    image.flags.writeable=False #image is no longer writable- saves memory\n",
    "    results=model.process(image)#process the keypoints in the image using mediapipe holistic. Basically prediction on the frame\n",
    "    image.flags.writeable=True\n",
    "    image=cv2.cvtColor(image,cv2.COLOR_RGB2BGR)#convert it back to BGR\n",
    "    return image,results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01e1568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results): #helper function to draw the lanmarks on the image\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, \n",
    "                                  mp_holistic.FACEMESH_CONTOURS,mp_drawing.DrawingSpec(color=(80,256,121),thickness=1,circle_radius=1),\n",
    "                                 mp_drawing.DrawingSpec(color=(110,256,121),thickness=1,circle_radius=1))#connects one landmark to another, contor is for the dots and connections is for the lines>\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "518fbec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0) #turns on the webcam, '0' is for webcam\n",
    "#access mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened(): #loop through all the frames\n",
    "        ret,frame=cap.read() #read the feed, feed is read one frame at atime. Frame is the image\n",
    "\n",
    "        #make detections\n",
    "        image,results=mediapipe_detection(frame,holistic)\n",
    "       # we have different types of functions for our results, landmark functions. for ex face landmark detects face keypoints nd left hand landmarks for left hand\n",
    "        \n",
    "        #draw landmarks\n",
    "        draw_landmarks(image,results)\n",
    "        \n",
    "        #show to screen\n",
    "        cv2.imshow('Open Feed',image) #showing the update 'frame'\n",
    "        \n",
    "        \n",
    "        if cv2.waitKey(10)&0xFF==ord('q'): #quit. Q on our keyboard\n",
    "            break\n",
    "    cap.release()#release the frames\n",
    "    cv2.destroyAllWindows()#closes the frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1fdd3f",
   "metadata": {},
   "source": [
    "# Extract Keypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca1cceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose=np.array([[res.x,res.y,res.z,res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "\n",
    "    lh=np.array([[res.x,res.y,res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    #what we are doing here is a bit of an error handling if any of the right hand or left hand array is empty we are filling it with zeroes of same shaep\n",
    "    #we knoe waxh hand landmark has 21 landmar elements comprising of 3 axes so (21*3)\n",
    "    #also list comprehension is used for insertion then the whole is converted to and array and flattened out\n",
    "\n",
    "    rh=np.array([[res.x,res.y,res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    #what we are doing here is a bit of an error handling if any of the right hand or left hand array is empty we are filling it with zeroes of same shaep\n",
    "    #we knoe waxh hand landmark has 21 landmar elements comprising of 3 axes so (21*3)\n",
    "    #also list comprehension is used for insertion then the whole is converted to and array and flattened out\n",
    "\n",
    "    face=np.array([[res.x,res.y,res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    #what we are doing here is a bit of an error handling if any of the right hand or left hand array is empty we are filling it with zeroes of same shaep\n",
    "    #we knoe waxh hand landmark has 468 landmar elements comprising of 3 axes so (468*3)\n",
    "    #also list comprehension is used for insertion then the whole is converted to and array and flattened out\n",
    "    \n",
    "    return np.concatenate([pose,face,lh,rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cabf63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test=extract_keypoints(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "facf71eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('0',result_test)#save the test extracted keypoint result in a file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb95345",
   "metadata": {},
   "source": [
    "# Folders for collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36945876",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=os.path.join('MP_Data')#path for exported keypoint data, numpy arrays\n",
    "\n",
    "actions=np.array(['hello','thanks','iloveyou'])#Actions that we will tryto detect\n",
    "\n",
    "no_seq=30 #30 videos worth of data\n",
    "\n",
    "seq_len=30#each video/seq will have 30 frames\n",
    "\n",
    "#the key diff between object detection and action det that we use multiple friends in the latter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdfac07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4487400"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total len of data\n",
    "1662*30*30*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c22462c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for action in actions:\n",
    "    for seq in range(no_seq):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(data_path,action,str(seq)))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "#creates directiories for eac h action and 30 seq\n",
    "#effectively MP_Data/hello/0.....MP_Data/Hello/29\n",
    "#and similar for the rest two\n",
    "#each seq(Folder 0..29) are gonna inturn contain 30 frames worth of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a0b883",
   "metadata": {},
   "source": [
    "# Collects keypoint values for training and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d8b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0) #turns on the webcam, '0' is for webcam\n",
    "#access mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    #loop thorugh actions\n",
    "    for action in actions:\n",
    "        #loop through sequences aka videos\n",
    "        for seq in range(no_seq):\n",
    "            #loop though video length\n",
    "            for frame_num in range(seq_len):\n",
    "                \n",
    "                \n",
    "        \n",
    "                ret,frame=cap.read() #read the feed, feed is read one frame at atime. Frame is the image\n",
    "\n",
    "                #make detections\n",
    "                image,results=mediapipe_detection(frame,holistic)\n",
    "               # we have different types of functions for our results, landmark functions. for ex face landmark detects face keypoints nd left hand landmarks for left hand\n",
    "                \n",
    "                #draw landmarks\n",
    "                draw_landmarks(image,results)\n",
    "                \n",
    "                #apply wait logic\n",
    "                if frame_num==0:\n",
    "                    cv2.putText(image,'STARTING COLLECTION',(120,200),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),4,cv2.LINE_AA)\n",
    "                    cv2.putText(image,f'Collecting frames for {action} video number {seq}',(15,12),cv2.FONT_HERSHEY_SIMPLEX,0.5,(125,255,178),1,cv2.LINE_AA)\n",
    "                    cv2.waitKey(2000)#if the frame is just starting add a collection break before the next sequence\n",
    "                else:\n",
    "                    cv2.putText(image,f'Collecting frames for {action} video number {seq}',(15,12),cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,0,0),2,cv2.LINE_AA)\n",
    "                \n",
    "                #Export keypoints\n",
    "                keypoints=extract_keypoints(results)\n",
    "                npy_path=os.path.join(data_path,action,str(seq),str(frame_num))\n",
    "                np.save(npy_path,keypoints)\n",
    "                #show to screen\n",
    "                cv2.imshow('Open Feed',image) #showing the update 'frame'\n",
    "        \n",
    "        \n",
    "                if cv2.waitKey(10)&0xFF==ord('q'): #quit. Q on our keyboard\n",
    "                    break\n",
    "    cap.release()#release the frames\n",
    "    cv2.destroyAllWindows()#closes the frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02779ab0",
   "metadata": {},
   "source": [
    "# Preprocess data and create labels and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ed453ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.23.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\srijan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9c265e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6d02868",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map={label:num for num,label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d602f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1956b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences,labels=[],[] #sequences=30 videos for each action\n",
    "for action in actions:\n",
    "    for seq in range(no_seq):\n",
    "        window=[] #create an empty list for each video to append every 30 frame containing 1662 keypoints\n",
    "        for frame_num in range(seq_len):\n",
    "            res=np.load(os.path.join(data_path,action,str(seq),f'{frame_num}.npy'))#loading up keypoints from each frame\n",
    "            window.append(res)#appending each frame with keypoints\n",
    "        sequences.append(window) #append the 30 frames to a video\n",
    "        labels.append(label_map[action]) #sets the labels for each action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ce2f0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array(sequences) # we got 90 vides(3*30, 30 videos for each action) each video has got 30 frames and eaxch frame has got 1662 keypoints\n",
    "#this is the x labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0e39426",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=to_categorical(labels).astype(int) #y labes using keras to convert the labels to flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eb3b54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y #100 represents hello, 010 thank you, 001 i love you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46a4383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "...     x, y, test_size=0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfd44a8",
   "metadata": {},
   "source": [
    "# Train the LSTM NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "250fbe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import LSTM,Dense,Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard,EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21d73f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir=os.path.join('logs')\n",
    "tb_callback=TensorBoard(log_dir=log_dir) #create a call back directory\n",
    "early=EarlyStopping(monitor='categorical_accuracy',patience=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e35aa536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(LSTM(64,return_sequences=True,activation='relu',input_shape=(30,1662))) #since there are 30 frames in each video with 1662 keypoints we will use it as input shape\n",
    "#return_seq is used as memory\n",
    "model.add(LSTM(128,return_sequences=True,activation='relu'))\n",
    "#model.add(Dropout(0.02))\n",
    "model.add(LSTM(64,return_sequences=False,activation='relu'))\n",
    "\n",
    "#model.add(Dropout(0.22))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dropout(0.018))\n",
    "model.add(Dense(actions.shape[0],activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c304709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c527a63b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "3/3 [==============================] - 9s 296ms/step - loss: 1.3121 - categorical_accuracy: 0.3529\n",
      "Epoch 2/200\n",
      "3/3 [==============================] - 1s 257ms/step - loss: 1.0788 - categorical_accuracy: 0.2235\n",
      "Epoch 3/200\n",
      "3/3 [==============================] - 1s 256ms/step - loss: 0.9677 - categorical_accuracy: 0.4941\n",
      "Epoch 4/200\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 1.1497 - categorical_accuracy: 0.4824\n",
      "Epoch 5/200\n",
      "3/3 [==============================] - 1s 244ms/step - loss: 2.2321 - categorical_accuracy: 0.3882\n",
      "Epoch 6/200\n",
      "3/3 [==============================] - 1s 245ms/step - loss: 0.9007 - categorical_accuracy: 0.4941\n",
      "Epoch 7/200\n",
      "3/3 [==============================] - 1s 263ms/step - loss: 0.9007 - categorical_accuracy: 0.4941\n",
      "Epoch 8/200\n",
      "3/3 [==============================] - 1s 261ms/step - loss: 0.8784 - categorical_accuracy: 0.4000\n",
      "Epoch 9/200\n",
      "3/3 [==============================] - 1s 253ms/step - loss: 0.7725 - categorical_accuracy: 0.4824\n",
      "Epoch 10/200\n",
      "3/3 [==============================] - 1s 254ms/step - loss: 0.6978 - categorical_accuracy: 0.6588\n",
      "Epoch 11/200\n",
      "3/3 [==============================] - 1s 259ms/step - loss: 0.6306 - categorical_accuracy: 0.6824\n",
      "Epoch 12/200\n",
      "3/3 [==============================] - 1s 255ms/step - loss: 0.6022 - categorical_accuracy: 0.6588\n",
      "Epoch 13/200\n",
      "3/3 [==============================] - 1s 262ms/step - loss: 1.1187 - categorical_accuracy: 0.4588\n",
      "Epoch 14/200\n",
      "3/3 [==============================] - 1s 314ms/step - loss: 0.6623 - categorical_accuracy: 0.6235\n",
      "Epoch 15/200\n",
      "3/3 [==============================] - 1s 440ms/step - loss: 0.6042 - categorical_accuracy: 0.6824\n",
      "Epoch 16/200\n",
      "3/3 [==============================] - 1s 434ms/step - loss: 0.6337 - categorical_accuracy: 0.6824\n",
      "Epoch 17/200\n",
      "3/3 [==============================] - 1s 436ms/step - loss: 0.6200 - categorical_accuracy: 0.6824\n",
      "Epoch 18/200\n",
      "3/3 [==============================] - 1s 449ms/step - loss: 0.5134 - categorical_accuracy: 0.8000\n",
      "Epoch 19/200\n",
      "3/3 [==============================] - 1s 408ms/step - loss: 0.3247 - categorical_accuracy: 0.8941\n",
      "Epoch 20/200\n",
      "3/3 [==============================] - 1s 438ms/step - loss: 1.4232 - categorical_accuracy: 0.5412\n",
      "Epoch 21/200\n",
      "3/3 [==============================] - 1s 485ms/step - loss: 0.7505 - categorical_accuracy: 0.6941\n",
      "Epoch 22/200\n",
      "3/3 [==============================] - 1s 463ms/step - loss: 0.6611 - categorical_accuracy: 0.6941\n",
      "Epoch 23/200\n",
      "3/3 [==============================] - 2s 551ms/step - loss: 0.6150 - categorical_accuracy: 0.6118\n",
      "Epoch 24/200\n",
      "3/3 [==============================] - 2s 538ms/step - loss: 0.5937 - categorical_accuracy: 0.7412\n",
      "Epoch 25/200\n",
      "3/3 [==============================] - 2s 522ms/step - loss: 0.6292 - categorical_accuracy: 0.6824\n",
      "Epoch 26/200\n",
      "3/3 [==============================] - 1s 384ms/step - loss: 0.5597 - categorical_accuracy: 0.7176\n",
      "Epoch 27/200\n",
      "3/3 [==============================] - 1s 448ms/step - loss: 0.7732 - categorical_accuracy: 0.6235\n",
      "Epoch 28/200\n",
      "3/3 [==============================] - 1s 433ms/step - loss: 0.6449 - categorical_accuracy: 0.6706\n",
      "Epoch 29/200\n",
      "3/3 [==============================] - 1s 442ms/step - loss: 0.7681 - categorical_accuracy: 0.6706\n",
      "Epoch 30/200\n",
      "3/3 [==============================] - 1s 410ms/step - loss: 0.6150 - categorical_accuracy: 0.6588\n",
      "Epoch 31/200\n",
      "3/3 [==============================] - 1s 429ms/step - loss: 0.6259 - categorical_accuracy: 0.6235\n",
      "Epoch 32/200\n",
      "3/3 [==============================] - 1s 495ms/step - loss: 0.5824 - categorical_accuracy: 0.6941\n",
      "Epoch 33/200\n",
      "3/3 [==============================] - 1s 415ms/step - loss: 0.5683 - categorical_accuracy: 0.8118\n",
      "Epoch 34/200\n",
      "3/3 [==============================] - 1s 404ms/step - loss: 0.4639 - categorical_accuracy: 0.8118\n",
      "Epoch 35/200\n",
      "3/3 [==============================] - 1s 425ms/step - loss: 0.5096 - categorical_accuracy: 0.6588\n",
      "Epoch 36/200\n",
      "3/3 [==============================] - 1s 415ms/step - loss: 0.4530 - categorical_accuracy: 0.8353\n",
      "Epoch 37/200\n",
      "3/3 [==============================] - 1s 455ms/step - loss: 0.4884 - categorical_accuracy: 0.7647\n",
      "Epoch 38/200\n",
      "3/3 [==============================] - 1s 427ms/step - loss: 0.3694 - categorical_accuracy: 0.8353\n",
      "Epoch 39/200\n",
      "3/3 [==============================] - 1s 431ms/step - loss: 0.3735 - categorical_accuracy: 0.8235\n",
      "Epoch 40/200\n",
      "3/3 [==============================] - 1s 439ms/step - loss: 0.3017 - categorical_accuracy: 0.9294\n",
      "Epoch 41/200\n",
      "3/3 [==============================] - 1s 428ms/step - loss: 0.4075 - categorical_accuracy: 0.8118\n",
      "Epoch 42/200\n",
      "3/3 [==============================] - 1s 431ms/step - loss: 0.2390 - categorical_accuracy: 0.8941\n",
      "Epoch 43/200\n",
      "3/3 [==============================] - 1s 442ms/step - loss: 0.3296 - categorical_accuracy: 0.8471\n",
      "Epoch 44/200\n",
      "3/3 [==============================] - 1s 419ms/step - loss: 0.3524 - categorical_accuracy: 0.8588\n",
      "Epoch 45/200\n",
      "3/3 [==============================] - 1s 450ms/step - loss: 0.2554 - categorical_accuracy: 0.8706\n",
      "Epoch 46/200\n",
      "3/3 [==============================] - 1s 453ms/step - loss: 0.2669 - categorical_accuracy: 0.9059\n",
      "Epoch 47/200\n",
      "3/3 [==============================] - 1s 433ms/step - loss: 0.2552 - categorical_accuracy: 0.8824\n",
      "Epoch 48/200\n",
      "3/3 [==============================] - 1s 426ms/step - loss: 0.1947 - categorical_accuracy: 0.9059\n",
      "Epoch 49/200\n",
      "3/3 [==============================] - 1s 422ms/step - loss: 0.1631 - categorical_accuracy: 0.9412\n",
      "Epoch 50/200\n",
      "3/3 [==============================] - 1s 428ms/step - loss: 0.1364 - categorical_accuracy: 0.9412\n",
      "Epoch 51/200\n",
      "3/3 [==============================] - 1s 409ms/step - loss: 0.1210 - categorical_accuracy: 0.9529\n",
      "Epoch 52/200\n",
      "3/3 [==============================] - 1s 455ms/step - loss: 0.2130 - categorical_accuracy: 0.9059\n",
      "Epoch 53/200\n",
      "3/3 [==============================] - 1s 467ms/step - loss: 0.2107 - categorical_accuracy: 0.9294\n",
      "Epoch 54/200\n",
      "3/3 [==============================] - 1s 494ms/step - loss: 0.3397 - categorical_accuracy: 0.8941\n",
      "Epoch 55/200\n",
      "3/3 [==============================] - 1s 440ms/step - loss: 0.2057 - categorical_accuracy: 0.9176\n",
      "Epoch 56/200\n",
      "3/3 [==============================] - 1s 427ms/step - loss: 0.1539 - categorical_accuracy: 0.9412\n",
      "Epoch 57/200\n",
      "3/3 [==============================] - 1s 420ms/step - loss: 0.1775 - categorical_accuracy: 0.9294\n",
      "Epoch 58/200\n",
      "3/3 [==============================] - 1s 415ms/step - loss: 0.1863 - categorical_accuracy: 0.9294\n",
      "Epoch 59/200\n",
      "3/3 [==============================] - 1s 424ms/step - loss: 0.1317 - categorical_accuracy: 0.9412\n",
      "Epoch 60/200\n",
      "3/3 [==============================] - 1s 444ms/step - loss: 0.1219 - categorical_accuracy: 0.9529\n",
      "Epoch 61/200\n",
      "3/3 [==============================] - 1s 464ms/step - loss: 0.0910 - categorical_accuracy: 0.9647\n",
      "Epoch 62/200\n",
      "3/3 [==============================] - 1s 456ms/step - loss: 0.1171 - categorical_accuracy: 0.9412\n",
      "Epoch 63/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtb_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#trained till 124 epochs as accuraccy was pretty good\u001b[39;00m\n",
      "File \u001b[1;32m~\\Dropbox\\PC\\Documents\\Programing\\Projects\\Python\\Deep_learning_proj\\Sign language detection\\signlanguage\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\Dropbox\\PC\\Documents\\Programing\\Projects\\Python\\Deep_learning_proj\\Sign language detection\\signlanguage\\lib\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\Dropbox\\PC\\Documents\\Programing\\Projects\\Python\\Deep_learning_proj\\Sign language detection\\signlanguage\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\Dropbox\\PC\\Documents\\Programing\\Projects\\Python\\Deep_learning_proj\\Sign language detection\\signlanguage\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\Dropbox\\PC\\Documents\\Programing\\Projects\\Python\\Deep_learning_proj\\Sign language detection\\signlanguage\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\Dropbox\\PC\\Documents\\Programing\\Projects\\Python\\Deep_learning_proj\\Sign language detection\\signlanguage\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2954\u001b[0m   (graph_function,\n\u001b[0;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Dropbox\\PC\\Documents\\Programing\\Projects\\Python\\Deep_learning_proj\\Sign language detection\\signlanguage\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1856\u001b[0m     args,\n\u001b[0;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1858\u001b[0m     executing_eagerly)\n\u001b[0;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\Dropbox\\PC\\Documents\\Programing\\Projects\\Python\\Deep_learning_proj\\Sign language detection\\signlanguage\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\Dropbox\\PC\\Documents\\Programing\\Projects\\Python\\Deep_learning_proj\\Sign language detection\\signlanguage\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train,y_train,epochs=200,callbacks=[tb_callback]) #trained till 124 epochs as accuraccy was pretty good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0786a130",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 30, 64)            442112    \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 30, 128)           98816     \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 596,675\n",
      "Trainable params: 596,675\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5556aadb",
   "metadata": {},
   "source": [
    "# Make predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "004db441",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22500bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.4855652e-01, 3.1455502e-01, 3.3688846e-01],\n",
       "       [9.7081135e-04, 1.6120598e-02, 9.8290861e-01],\n",
       "       [1.8294715e-05, 9.9941278e-01, 5.6892732e-04],\n",
       "       [1.6121319e-05, 9.9933547e-01, 6.4838812e-04],\n",
       "       [8.6418194e-01, 1.2694535e-01, 8.8727018e-03]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "462faff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(res[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f8160e8-8546-423f-be33-7d70b6748917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6b191603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(y_test[4])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3a2b79",
   "metadata": {},
   "source": [
    "#### Save the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4579b32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action2.h5')#save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94a34040-3eba-458c-ade3-938aec85bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b66d2d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "action_model = tf.keras.models.load_model('action2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049c9d1e",
   "metadata": {},
   "source": [
    "# Evaluating confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf2f7be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e5bff594",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat=action_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c1b85be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue=np.argmax(y_test,axis=1).tolist()\n",
    "yhat=np.argmax(yhat,axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99c9875a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3, 0],\n",
       "        [2, 0]],\n",
       "\n",
       "       [[0, 4],\n",
       "        [0, 1]],\n",
       "\n",
       "       [[3, 0],\n",
       "        [2, 0]]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue,yhat) #represents the matrices for 3 categories hello,thankyou and iloveyou\n",
    "#matrix is arranged in true +ve,False +ve,False -ve,True n-ve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6ebf5cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01afd05",
   "metadata": {},
   "source": [
    "# Test in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f7731f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new detection variables\n",
    "sequence=[]\n",
    "sentence=[]\n",
    "threshold=0.73\n",
    "\n",
    "\n",
    "\n",
    "cap=cv2.VideoCapture(0) #turns on the webcam, '0' is for webcam\n",
    "#access mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened(): #loop through all the frames\n",
    "        ret,frame=cap.read() #read the feed, feed is read one frame at atime. Frame is the image\n",
    "\n",
    "        #make detections\n",
    "        image,results=mediapipe_detection(frame,holistic)\n",
    "        #print(results)\n",
    "       # we have different types of functions for our results, landmark functions. for ex face landmark detects face keypoints nd left hand landmarks for left hand\n",
    "        \n",
    "        #draw landmarks\n",
    "        draw_landmarks(image,results)\n",
    "\n",
    "        #prediction logic\n",
    "        while True:\n",
    "        \n",
    "        \n",
    "            keypoints= extract_keypoints(results)\n",
    "            \n",
    "            sequence.insert(0,keypoints)\n",
    "            \n",
    "            sequence=sequence[:30]\n",
    "\n",
    "            if(len(sequence)==30):break\n",
    "           \n",
    "    \n",
    "        res=action_model.predict(np.expand_dims(sequence,axis=0))[0]\n",
    "        #print(actions[np.argmax(res)])\n",
    "\n",
    "        #viz. logic\n",
    "        if res[np.argmax(res)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "        \n",
    "\n",
    "        if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            #image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "       \n",
    "        \n",
    "        #show to screen\n",
    "        cv2.imshow('Open Feed',image) #showing the update 'frame'\n",
    "        \n",
    "        \n",
    "        if cv2.waitKey(10)&0xFF==ord('q'): #quit. Q on our keyboard\n",
    "            break\n",
    "    cap.release()#release the frames\n",
    "    cv2.destroyAllWindows()#closes the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbb1620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e56309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signlanguage",
   "language": "python",
   "name": "signlanguage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
